---
layout: post
title: 面试准备
---

- Flink
- XGBoost
- 深度学习



## 机器学习基础知识

#### AUC：

ROC曲线（TPRate-FPRate）下的面积

TPRate==FPRate的意义：即AUC==0.5，分类器对于正例和负例没有区分能力，和抛硬币没有区别

期望：分类器 __将真实类别为1的样本预测为1的概率TPRate__ 尽可能大于 __将真实类别为0的样本预测为1的概率FPRate__

理想：AUC==1

典型场景：类别不平衡数据集上的模型评估

#### SMOTE

超采样技术，为minority class通过插值合成新样本（通过KNN找到近邻，求平均gap找到插值点）。

### 经典模型

#### XGBoost

相比GBDT的优化点：

- 损失函数和基分类器。利用二阶泰勒展开推广了加性模型的Loss function（因为是加性模型，才能做泰勒展开），从而支持自定义损失函数和基分类器
  - 自定义损失函数要有一阶和二阶导数（实现Loss的二阶泰勒近似）
  - 基分类器应是线性分类器（简单）

加性模型的被动效果是每轮新增的模型都在学习之前预测的残差。

- 防止过拟合。
  - 损失函数增加了正则项，从树中 __叶子节点个数T__ 和 __叶子节点得分w的L2范数__ 两个角度限制模型复杂度
  - 列采样
  - 样本采样
  - shrinkage
- 默认值处理。
  - 寻找分裂点时不遍历缺失值
  - 将所有缺失值放入左子节点和右子节点各算一遍，选择增益最大的一个
  - 训练中没有缺失值而预测时有，自动将缺失值划分到右子节点
- 并行化。特征并行，将每个特征预排序并存储为Block，并行查找每个特征的分割点
- 停止生长条件。
  - 分裂带来的增益小于阈值
  - 达到最大深度
  - 样本权重和小于阈值

XGB与LGB的区别：



#### Word2Vec

理论基础：分布式假设

优点：可以衡量语义相似度

缺点：无法解决一词多义（上下文无关）

网络结构：类似自编码器，隐层做compression，decoder的对象不是自己而是上下文词

##### CBOW

- 多对一

- 适合小数据集，更加平滑（summation）
- 输入是one-hot vector，输出是one-hot vector

##### skip-gram

- 一对多

- 适合大数据集（参数更多？？？）
- 输入是one-hot vector，输出是multi-hot vector

上述结构最大的问题是，输出层为softmax，其分母必须考虑词表中所有的词，当词表很大时效率低

训练优化技术：

- hierarchical softmax，低频词
- negative sampling

Huffman tree



#### Glove



#### DeepFM

解决一词多义（上下文有关）

#### DeepWalk



#### Transformer



#### Bert



#### 我关心的问题

- 薪酬结构
- 绩效和晋升
- 加班情况
- 团队管理模式
